PRASHANT D. JOSHI
UT Computer Science, The University of Texas at AustinFLOATING POINT FORMATS AND 
DESIGN OF ADDERS AND 
MULTIPLIERS
June 5th, 2025


Todayâ€™s Agenda
1. Need for non -integral value representations
2. Representing fractions: Fixed point vs Floating point
3. FP Addition
4. FP Multiplication
5. Faster algorithms
6. Summary


Why Floating Point?
â€¢Recent high -performance microprocessors are all 64 -bit machines.
â€“18,446,744, 073,709,600,000 distinct values. (Approx 18*1018)
â€“Split these distinct values to represent integers only? Fractions also?
â€¢Physics and mathematics regularly deal with numbers beyond these ranges. 
â€“Planckâ€™s constant is 6.626, 070, 15 Ã—10âˆ’34 J.s.
â€“Avogadroâ€™s number is 6.022, 140, 76 Ã— 1023 mol-1.
â€“Electron mass: 9.109 x 10-31 kg
â€“Neutrino mass upper bound: 10-37 kg
â€“Yearly internet data traffic: > 1021 bytes
â€¢We need representations for such quantities that are efficient in storage space, 
computation time, precision, and easy to design in hardware

Non- integral representations
â€¢Decimal representation: dn-1 dn-2 â€¦ d1 d0. d-1 d-2 â€¦ d-m
 results in a value: âˆ‘ğ‘–ğ‘–=âˆ’ğ‘šğ‘šğ‘–ğ‘–=ğ‘›ğ‘›âˆ’1ğ‘‘ğ‘‘ğ‘–ğ‘–âˆ—10ğ‘–ğ‘–
â€¢Binary representation : bn-1 bn-2 â€¦ b1 b0. b-1 b-2 â€¦ b-m
       (binary or radix point) results in a value: âˆ‘ğ‘–ğ‘–=âˆ’ğ‘šğ‘šğ‘–ğ‘–=ğ‘›ğ‘›âˆ’1ğ‘ğ‘ğ‘–ğ‘–âˆ—2ğ‘–ğ‘–
â€¢Both representations have n  digits of integer 
value and m digits for the fractional value

Examples of Non-Integral Values
â€¢11
2=5.510=101. 12
â€¢1
10=0.110=0.000112
â€¢1
3=0.310=0.012
â€¢ Constraint : We can have only a fixed number of digits.
â€¢ Implication : We canâ€™t represent all numbers exactly.
Thatâ€™s obvious for 
irrationals, but notice it 
is true for rationals  also
for example : 0.00011  ïƒ  0.00011001100110011â€¦
Where do we stop? Limited digitsâ€¦ resolution


Scientific Floating Point representation
â€¢Scientific notation (also called exponential or standard form representation)
â€¢Extend the â€œ normalized â€ (base -10) scientific notation:
â€“Represent ğ‘¥ğ‘¥ asÂ±ğ‘†ğ‘† Ã— 10ğ¸ğ¸, with 
ğ‘†ğ‘†=(ğ‘‘ğ‘‘0â‹…ğ‘‘ğ‘‘1â‹¯)10 where 1â‰¤ğ‘‘ğ‘‘0<10.
â€“ğ‘†ğ‘†: significand/mantissa;  ğ¸ğ¸: exponent.
â€“The representation of the value 11
2 is uniquely  +5.5 Ã— 100
â€¢ not+55.0 Ã— 10âˆ’1, or +0.55 Ã— 101, or any other possibility.
â€“Why?


From Base- 10 to Base- 2
â€¢Represent ğ‘¥ğ‘¥ asÂ±ğ‘†ğ‘† Ã— 2ğ¸ğ¸, with 
ğ‘†ğ‘†=(ğ‘ğ‘0â‹…ğ‘ğ‘1â‹¯)2where  ğ’ƒğ’ƒğŸğŸ=ğŸğŸ. (since it must be 1 â‰¤ ğ‘ğ‘0<2, per previous slide)
â€“ğ‘†ğ‘† =(1â‹…ğ‘ğ‘1 ğ‘ğ‘2â‹¯ ğ‘ğ‘ğ‘ğ‘âˆ’1)2 (having ğ‘ğ‘ bits in all, of which ğ‘ğ‘0=1).
â€“Likewise, ğ¸ğ¸ will be represented in ğ‘ğ‘  bits; encoding format described 
shortly.
â€¢Call the set of representable numbers ğ”½ğ”½ğ”½(ğ‘ğ‘,ğ‘ğ‘;2).
â€¢Note : We canâ€™t represent zero as a normalized FP number. 
Why?


Value of represented points
â€¢Between two consecutive exponent values, what is the number of 
mantissa representations?
â€“The same?
â€¢What is the â€˜valueâ€™ between two consecutive represented points (between the same exponents and across different exponents)?
1      2     4       8                                     256 0.125  0.25      0.5 
Finer resolution Coarser resolution
Where is zero?
How to 
represent it?

Now on to the Exponent
â€¢We represent ğ¸ğ¸ using ğ‘ğ‘ bits.
â€“Need positive and negative values of ğ¸ğ¸. Why?
â€“Use â€˜biasedâ€™ or â€˜excessâ€™ representation
â€¢Ease of manipulation
â€“bias =2ğ‘ğ‘âˆ’1âˆ’1
â€¢If the binary pattern of the q binary bits reads X, the value* 
in the biased notation is X - bias
*: Distinction between representation and value

Normalized FP numbers
bias =2ğ‘ğ‘âˆ’1âˆ’1.
â€¢ The minimum representable exponent:  
ğ¸ğ¸min =0ğ‘ğ‘âˆ’11=1âˆ’bias =âˆ’(biasâˆ’1).
â€¢ The minimum normalized positive FP number:
ğ‘ğ‘min =1.0ğ‘ğ‘âˆ’12Ã—2ğ¸ğ¸min.
â€¢ The maximum representable exponent:  
ğ¸ğ¸max= 1ğ‘ğ‘âˆ’10=bias .
â€¢ The maximum normalized positive FP number:  
ğ‘ğ‘max =(1.1ğ‘ğ‘âˆ’1)2Ã—2ğ¸ğ¸max =(2âˆ’2âˆ’(ğ‘ğ‘âˆ’1))Ã—2ğ¸ğ¸maxâ‰ˆ2(ğ¸ğ¸max+1).
â€¢ We will not use the two patterns 0ğ‘ğ‘ and 1ğ‘ğ‘ for normalized FP numbers, reserving 
them for special use .


The Complete Representation
â€¢Sign bit: 0  for positive, 1 for negative.
â€¢We use ( 1+ğ‘ğ‘+(ğ‘ğ‘âˆ’1))=ğ‘ğ‘+ğ‘ğ‘ bits, arranged as 
above.
â€“Although the significand has ğ‘ğ‘  bits, we know that ğ‘ğ‘0=1. 
So, we donâ€™t store it explicitly. This trick to save storage is 
called the â€œ hidden bit â€. (Also called the â€˜impliedâ€™ bit)
If my precision is of p bits, why do I have 
only ( p-1) bits in the Significand?


IEEE 754 Formats
Format Name Single -Precision Double -Precision
Precision, p (bits) 24 53
Exponent width, ğ‘ğ‘ (bits) 8 11
Total size, ğ‘ğ‘+ğ‘ğ‘ (bits) 32 64
Associated C data type float double
Exponent bias value 28âˆ’1âˆ’1=127 211âˆ’1âˆ’1=1023
ğ¸ğ¸min, ğ¸ğ¸max âˆ’126 ,127 âˆ’1022 ,1023
ğ‘ğ‘min, ğ‘ğ‘max (binary)1Ã—2âˆ’126,(2âˆ’2âˆ’23)Ã—
 2127 1Ã—2âˆ’1022,(2âˆ’2âˆ’52)Ã—
 21023 
ğ‘ğ‘  ğ‘ğ‘   â‰ˆ1.2Ã—10âˆ’38, â‰ˆ2.2Ã—10âˆ’308,
(1, 8, 23) (1, 11, 52)


How To Represent Zero
â€¢Since zero is unique, we deal with it as a special case.
â€¢Hey, the value 0  will end up with two distinct FP 
representations: âˆ’0=1ğŸğŸğ’’ğ’’0ğ‘ğ‘âˆ’1, and +0=0ğŸğŸğ’’ğ’’0ğ‘ğ‘âˆ’1 !
â€¢Note that this uses up one of the two patterns that we 
disallowed for normalized FP numbers (0ğ‘ğ‘âˆ’1) .
â€¢All 0s in the exponent will be used for special number representations, zero being one of them
Are we not wasting a representation for +0 and -0?
It is on purpose!!! Sometimes in mathematics it is important to know if we are approaching 
0 from the positive or negative side and this is a big help!

Subnormal Numbers
â€¢Extend the definition
0ğ‘ğ‘=ğ¸ğ¸min. Note, not ğ¸ğ¸minâˆ’1.
â€¢Now, ifğ¸ğ¸=0ğ‘ğ‘, then treat the hidden bit ğ‘ğ‘0as 0 rather 
than 1in calculating the value.
ğ‘ ğ‘ ğŸğŸğ’’ğ’’ğ‘ğ‘1â‹¯ğ‘ğ‘ğ‘ğ‘âˆ’1=Â±(ğŸğŸ.ğ‘ğ‘1â‹¯ğ‘ğ‘ğ‘ğ‘âˆ’1)2Ã—2ğ‘¬ğ‘¬ğ¦ğ¦ğ¦ğ¦ğ¦ğ¦
â€¢Note that the earlier rule for zero is a special case of 
this more general rule.Unsigned Repr Biased
0 0000 -6
1 0001 -6
2 0010 -5
3 0011 -4
4 0100 -3
5 0101 -2
6 0110 -1
7 0111 0
8 1000 1
9 1001 2
10 1010 3
11 1011 4
12 1100 5
13 1101 6
14 1110 7
15 1111 8
(from before)
What is the gap length between the consecutive values between 0 and 1 considering both the subnormal and normal numbers?


Subnormals  in IEEE 754
Format Name Single- Precision Double -Precision
Precision, p (bits) 24 53
Exponent width, ğ‘ğ‘ (bits) 8 11
ğ¸ğ¸min âˆ’126 âˆ’1022
ğ‘ğ‘min (binary) 1Ã—2âˆ’1261Ã—2âˆ’1022
Largest + ve subnormal (0.123)2Ã—2âˆ’126=2âˆ’126âˆ’2âˆ’149 (0.152)2Ã—2âˆ’1022
=2âˆ’1022âˆ’2âˆ’1074
Smallest + ve subnormal (0.0221)2Ã—2âˆ’126=2âˆ’149(0.0511)2Ã—2âˆ’1022=2âˆ’1074
# of +ve  subnormals 223252
Subnormal spacing 2âˆ’1492âˆ’1074


Infinities
â€¢We still have some bit patterns left over!! (all 1s in E)
â€¢Just as we used a special representation for zero, 
we are going to use special representations forÂ±âˆ.
â€“The pattern 0 ğŸğŸğ’’ğ’’0ğ‘ğ‘âˆ’1will represent the value +âˆ.
â€“The pattern 1 ğŸğŸğ’’ğ’’0ğ‘ğ‘âˆ’1 will represent the value âˆ’âˆ.
â€¢We still have some bit patterns left over. 
Unsigned Repr Biased
0 0000 Sub. ( -6)
1 0001 -6
2 0010 -5
3 0011 -4
4 0100 -3
5 0101 -2
6 0110 -1
7 0111 0
8 1000 1
9 1001 2
10 1010 3
11 1011 4
12 1100 5
13 1101 6
14 1110 7
15 1111 Â±âˆ

Not a Number ( NaN)
â€¢What are the results of â„10, â„1âˆ, âˆ+âˆ?
â€“These situations are well -defined (in terms of limits):  âˆ, 0, âˆ.
â€¢How about  0Ã—âˆ, â„00, âˆâˆ’âˆ? How about taking the square root of a 
negative number?
â€“These situations are either ill -defined (think in terms of sequences and their 
convergence/divergence in the limit) or undefined (for square root).
â€¢IEEE 754â€™s response is that these operations should yield â€œNot a Numberâ€ 
(NaN ), and that NaNs  must propagate through future operations .
â€“Any representation with E =1ğ‘ğ‘ but significand â‰  0ğ‘ğ‘âˆ’1 is a NaN.
â€“These multiple NaNs  are unordered .
How many possible 
representations will evaluate 
to a NaN ?


What question may I answer before 
we continue?
Remember, all questions are good questions!

Correctly Rounded Value (IEEE 754)
â€¢For ğ‘¥ğ‘¥âˆˆâ„+ : 
If ğ‘¥ğ‘¥âˆˆğ”½ğ”½ğ”½ğ‘ğ‘,ğ‘ğ‘, ROUND (ğ‘¥ğ‘¥)=ğ‘¥ğ‘¥. 
Otherwise, ROUND (ğ‘¥ğ‘¥) depends on the prevailing rounding mode .
â€“ Round down (RD): ROUND (ğ‘¥ğ‘¥)=ğ‘¥ğ‘¥âˆ’.
â€“ Round up (RU) :ROUND (ğ‘¥ğ‘¥)=ğ‘¥ğ‘¥+.
â€“ Round- to-zero (RTZ): ROUND (ğ‘¥ğ‘¥)=if ğ‘¥ğ‘¥>0 then  ğ‘¥ğ‘¥âˆ’ else  ğ‘¥ğ‘¥+.
â€“ Round- to-nearest, with tiebreak to even (RTE): ROUND (ğ‘¥ğ‘¥)= whichever of ğ‘¥ğ‘¥âˆ’ or ğ‘¥ğ‘¥+ is closer 
to ğ‘¥ğ‘¥ (sometimes written âŒŠğ‘¥ğ‘¥âŒ‰). 
In case of a tie, choose the one whose significand has lsb0. [Default]
â€¢Useful in numerical analysis
â€¢ Additional bits
â€“ Round, Guard, Sticky bits
â€“ Used to reduce errors due to approximations

Guard, Round, Sticky
â€¢IEEE 754 rules require rounding to 
nearest representable value with ties broken per settings
â€¢FP intermediate values are almost always exceeding the precision formats (>24 bits single precision, >53 double)
â€¢GRS help with tracking lost precision beyond the mantissa field
â€“Maintain a few more bits in the design though end representation will be per IEEE 754 rulesGuard: Immediate bit(s) beyond 
mantissa LSBRound: The bit after Guard bit(s)Sticky: OR of all remaining lower order bits
Whatâ€™s the benefit 
of more than 3 
GRS bits?
Possible rule:
<mantissa>GRS<mantissa>1x1 ïƒ  Round up
<mantissa>0xx ïƒ  Truncate
<mantissa 1>100  ïƒ  round to even Mantissa GRS
Mantissa GGGRS

Floating Point Addition
â€¢Need to make exponents equal to add
â€“3.141 * 100 + 9.718 * 101
â€¢Right shift fraction part of smaller exponent until equal 
to larger
â€“0.314 * 101 + 9.718 * 101
â€¢Add the mantissas
â€“10.032 * 101
â€¢Renormalize if necessary
â€“1.003 * 102Still normalized?4. Round the significand to the appropriate
number of bitsYes Overflow or
underflow?Start
No
Yes
Done1.  Compare the exponents of the two numbers.
Shift the smaller number to the right until its
exponent would match the larger exponent
2. Add the significands
3. Normalize the sum, either shifting right and
incrementing the exponent or shifting left
and decrementing the exponent
No Exception

FP Addition Block 
Diagram


Flowchart for 
FP Adder
Courtesy: Dr. Lizy John, UT Austin?  

FP Adder sub- blocks
â€¢Comparator
â€¢Shift register for shifting smaller number to right
â€¢Integer adder, possibly 2s complement (after converting of 
course!)
â€¢Bidirectional shifter to increment/decrement result
â€¢Additional hardware for overflow, underflow, rounding detection
â€¢Control sub -block to enable above steps

Implementable pseudocode FADD
Input: Floating point numbers A and B, each in (sign, exponent, mantissa) 
format
Output: Floating point number S = A + B in (sign, exponent, mantissa) 
format
1. Unpack A and B:
   (sign_A, exp_A , mant_A ) = unpack(A)
   (sign_B , exp_B , mant_B ) = unpack(B)
   // Add implicit 1 to mantissas (normalized numbers)
   mant_A  = 1.mant_A
   mant_B  = 1.mant_B
2. Align exponents:
   If exp_A  > exp_B :
       shift = exp_A  - exp_B
       mant_B  = mant_B  >> shift
       exp_result  = exp_A
   Else:
       shift = exp_B  - exp_A
       mant_A  = mant_A  >> shift
       exp_result  = exp_B3. Add/Subtract mantissas:
   If sign_A  == sign_B :
       mant_result  = mant_A  + mant_B
       sign_result  = sign_A
   Else:
       If mant_A  > mant_B :
           mant_result  = mant_A  - mant_B
           sign_result  = sign_A
       Else:           mant_result  = mant_B  - mant_A
           sign_result  = sign_B
4. Normalize the result:   If mant_result  has carry -out (i.e., MSB is 2 bits wide):
       mant_result  = mant_result  >> 1
       exp_result  = exp_result  + 1
   Else:
       While mant_result  < 1.0 and exp_result  > 0:
           mant_result  = mant_result  << 1
           exp_result  = exp_result  - 1
5. Round the result (optional; implement rounding to nearest even)   Apply rounding on mant_result  if using guard, round, sticky bits
6. Pack result:   Remove implicit leading 1 from mantissa
   Return floating_point (sign_result , exp_result , mant_result [lower 23 bits])

FP Adder
â€¢Much more complex than Integer Add
â€¢Takes more time as well
â€¢Can we â€œpipelineâ€ this? 
â€“Potentially break up the tasks into stages and pipeline them
â€“Compare Exponents
â€“Shift
â€“Mantissa addition
â€“Normalization and Rounding
â€¢Useful when multiple back-to -back additions are often done
How could this 
impact a single 
add, not multiple 
back -to-back 
adds?

Floating Point Multiplication
â€¢Consider and example: 1.110 * 1010 X 9.200 * 10-5
â€“Add exponents (for biased exponents, subtract bias 
from sum as it is double counted)
â€¢New Exponent = 10 - 5 = 5
â€“Multiply Significands
â€¢1.110 X 9.200 = 10.212 ïƒ¨10.212 * 105
â€“Normalize, Round result
â€¢1.021 * 106
â€“Determine sign of result from operands
â€¢+1.021 * 106


Multiplication
â€¢Long hand multiplication
â€“Note the Multiplier shifts right, while the multiplicand 
shifts left


Implementable pseudocode FMUL
Classic Long Hand method
Input: Floating point numbers A and B, each in (sign, exponent, mantissa) 
format
Output: Floating point number P = A Ã—  B in (sign, exponent, mantissa) 
format
1. Unpack A and B: Extract sign, exp, mantissa
   (sign_A, exp_A , mant_A ) = unpack(A)
   (sign_B , exp_B , mant_B ) = unpack(B)
   // Add implicit leading 1 to both mantissas
   mant_A  = 1.mant_A   // 24 bits
   mant_B  = 1.mant_B   // 24 bits
2. Compute result sign: If both signs are equal result will be positive
   sign_P  = sign_A XOR sign_B
3. Compute raw exponent: Remember if you are doing this without 
conversion to 2â€™s complement, then you are double counting the bias, 
hence subtract it once
   exp_P  = exp_A  + exp_B  - Bias   // Bias = 127 for IEEE 754 single precision4. Multiply mantissas: this requires implementing the previous slideâ€™s logic
   mant_P  = mant_A  Ã— mant_B        // 24 Ã— 24 = 48 bits
5. Normalize mantissa: Adjust result to be of the form 1.xxxxâ€¦x
   If mant_P [47] == 1:
       // Already normalized (leading bit in position 47)
       mant_P  = mant_P  >> 24      // Take top 24 bits (1.xxx...)
       exp_P  = exp_P  + 1
   Else:
       mant_P  = mant_P  >> 23      // Align to 1.xxx... if MSB is at position 46
6. Round mantissa (optional): If you are using GRS bits then round 
appropriately
   Apply rounding if needed using guard/round/sticky bits
7. Pack result: Reassemble the sign, exp, mantissa into final resut
   Store sign_P  as sign bit
   Store exp_P  as 8-bit exponent
   Store lower 23 bits of mant_P  as mantissa (remove implicit 1)
   8. Return floating_point (sign_P , exp_P , mant_P [22:0])
What can be the max value after step 4?

Mantissa Multiplication pseudocode
Step 4 from previous slide
â€¢The multiplier  is processed bit by 
bit from LSB to MSB .
â€¢The multiplicand  is left -shifted 
each cycle.
â€¢If the current LSB of multiplier  is 
1, the multiplicand is added to 
the partial product .
â€¢After each step, the multiplier is right -shifted to examine the next 
bit.Input: 
  multiplicand : 32-bit unsigned integer
  multiplier   : 32-bit unsigned integer
Output:
  partial_prod  : 64-bit unsigned integer
1. Initialize:   partial_prod  = 0
multiplicand_64 = zero -extend multiplicand to 64 bits
2. For i  = 0 to 31:
     If (multiplier AND 1) == 1:         partial_prod  = partial_prod  + multiplicand_64
     EndIf
     multiplicand_64 = multiplicand_64 << 1
     multiplier = multiplier >> 1   // logical shift rightEndFor


Optimized multiplication
â€¢Perform steps of add/shift in parallel
â€¢If we are doing a 4 bit X 4 bit multiplier, do we need an 8 bit adder?
â€“No
â€“Instead shift the partial product to the right each time
â€“Also are we using the full 8 bit result in the first step? No!
â€¢Use the final resultâ€™s register to keep the partial product AND the multiplier! One gets shifter 
out, one gets shifted in so the length suffices! Saves some hardware!
00001001
ïƒ 0100010001000100ïƒ  00100010 0010001 0
ïƒ  0001000100010001ïƒ  01001000

Faster Multipliers
â€¢Use multiple adders
â€“Cost/Performance tradeoff
â€¢Can be pipelined, with several multiplications in parallel


What question may I answer before 
we end â€¦?
Remember, all questions are good questions!

Backup slides

1/10th in binary
Decimal to Binary Conversion
1. Start with the decimal fraction 1/10=0.1
2. Multiply 0.10 by 2: 0.1Ã— 2=0.20
3. The integer part is 0 . Write down 0.
4. Multiply the remaining fraction 0. 20 by 2: 0.2Ã— 2=0.40
5. The integer part is 0 . Write down 0.
6.Multiply the remaining fraction 0. 40 by 2: 0.4Ã— 2=0.80
7. The integer part is 0. Write down 0.
8.Multiply the remaining fraction 0.80 by 2: 0.8 Ã—2=1.60
9. The integer part is 1 . Write down 1. Now, the remaining fraction is 0.60
â€¦
And so on
â€¦
During the Gulf War of 1991, a US Patriot missile failed to intercept a Scud missileâ€¢A later study showed the problem was due to the 
inaccuracy of the binary representation of 1/10
â€¢The Patriot system incremented the counter every 
0.1 seconds, and multiplied the counter value by 0.1 to compute the actual time
â€¢The 24 -bit binary number of 0.1 is 
0.099999904632568359375, which is off by 0.000000095367431640625
â€¢Big deal, right?
â€¢Nope!
â€¢After about 4 days, this error starts to add up to 0.34 seconds which allows the Scud to travel 500 meters
â€¢This caused the failure!

Other IEEE 754 Formats
Courtesy Dr. Lizy John, UT Austin