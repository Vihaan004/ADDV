06/15/2025  1SoC Verification
Brendan Donahe
28 years of experience mostly in verification but also 
design and software
Part of teams at well-known larger companies such 
as IBM, Motorola, Conexant, and NVIDIA as well as 
verification lead at 5 different small startups

  
 Let’s talk briefly about the main goals of this lecture.
First, I’d like you all to be able to walk away with a 
high level understanding of the SoC verification 
process.
Second, learn how to choose the “right tool for the 
job” from the array of methodologies at your 
disposal.
Third, consider efficiency and reuse both within your 
team and across teams and within a project and 
across multiple projects.

  
 Much like hardware design languages promote 
hierarchy and reuse of components, verification of 
complex SoCs also focus in various areas, 
sometimes specific to a single, complicated unit 
and other times a collection of units or subsystems. 
Generally as verification moves from unit to system, 
the smaller, more focused bench components, 
checkers, and even stimulus can potentially be 
reused at the higher levels.

  
  
Here we have a table showing various pros and cons 
of doing verification at various levels of hierarchy.  
At the bottom, unit verification often enables the use 
of very specific, custom models that can be used 
for checking that can also handle a lot of 
randomization.
At the top, full chip verification focuses on higher-
level interactions and connectivity through less-
randomized use case tests.  Often, there is great 
potential for reuse. 

  
 

  
 …
Video encode/decode engines, in particular, are often 
complex enough that design and debug is broken 
down into smaller submodules.

  
 

  
 Here is a typical UVM-oriented unit testbench.  In this 
context, each BFM or Bus Functional Model has the job 
of speaking a single protocol to the DUT or Design Under 
Test.  This particular DUT has only 2 interfaces, perhaps 
a standard internal bus protocol on one side and a serial 
protocol which may go off-chip on the other.  
Inside each agent, stimulus is generated by the sequencer.  
It creates the transaction which may have many attributes 
such as data or address and even delays between 
transactions.  
The driver sends signals to the DUT and comprehends 
signals from the DUT so that it can react appropriately.
The monitor also comprehends the protocol and contains 
protocol checkers that understand both sides of the 
driver/DUT conversation.
The scoreboard or checker understands how the DUT is 
supposed to work internally and confirms that the data 
out of the DUT looks correct given inputs.
With respect to reuse, it’s very important to note that the 
scoreboard operates only on information from the 
monitors (which is shared via “analysis ports”).

  
 

  
 

  
 Here is an example subsystem testbench which 
shows a collection of 3 associated units.  The 
interfaces at the top level of this subsystem are 
connected to active agents, but the internal 
interfaces of one of the units are also being 
monitored by agents that are inactive or passive, 
perhaps sourced from the unit verification 
testbench.
Note that in theory, the passive agents’ monitors and 
the scoreboard they’re connected to shouldn’t find 
any issues assuming unit verification with these 
components was thorough, HOWEVER, sometimes 
with additional delays through the bus or other I/O 
interfaces, problems can be detected here that 
were missed at unit level, for example.  Additionally, 
these monitors can aid in isolation of issues when 
debugging.

  
 Here’s a real-life example subsystem testbench that I 
put together for verification of our in-house 
designed AXI fabric, EBI controller, and internal 
SRAM.  In this situation, the purchased IPs were 
the eXecute-In-Place SPI controller, the CPUs, and  
DMA controller.  In this environment, I’ve replaced 
the CPUs and DMA controller with my own AXI 
BFM agents which can drive much more interesting 
and stressful transactions than I can easily 
configure the DMA controller to do or write C or 
assembly software to have the CPUs do.

  
 

  
 

  
 Here’s an example full chip top level testbench which 
shows a mix of active and passive agents.  
On the left, an active peripheral agent drives input or 
responses to a device in the peripheral subsystem 
and a passive agent monitors communication 
between the peripheral and the system bus.  This is 
checked via the scoreboard that sits between these 
two agents.
On the right, an active memory agent BFM responds 
to requests from a memory controller while the 
active agent monitors communication between the 
memory controller and the system bus.  This is 
check via the scoreboard that sits between these 
two agents.

  
 

  
 

  
 

  
 

  
 

  
 

  
 

  
 

  
 Now let’s put together a top level testbench taking into 
account the prior set of topics.  Of note here:
This CPU BFM can hold the CPU RTL in reset and drive  
transactions in place of it.
If the CPU RTL is present, the PC monitor logs the program 
counter values for future boot ROM code coverage 
analysis.
Preloaders are present that enable us to drop code and 
data in appropriate places in memory for a CPU or DMA 
controller to access.
Memory, HDMI, and serial device models are present to 
communicate with appropriate on-chip devices through 
their external pins’ pads.  (Proving connectivity and 
appropriate pinmuxing, input, or output enable 
configuration through these is important.)
Some peripherals have been looped back to themselves.
The trick box is attached to the chip via XMR probes so that 
the CPU can talk through it to the rest of the bench.
There’s a simple reset controller that can assert 
“pushbutton” reset.
There’s an XTAL model driving clocks into the SoC.